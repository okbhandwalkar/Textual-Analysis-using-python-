{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db2221a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lessons from the past: Some key learnings relevant to the coronavirus crisis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "url = \"\"\"https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis/\"\"\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"}\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "title=soup.find('h1',class_=\"entry-title\")\n",
    "title=title.text.replace('\\n',\" \")\n",
    "title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f79d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " So, not beginning with once upon a time because it is not a fairytale. Before the advent of this deadly virus called Coronavirus or with all love that we might want to call it COVID-19, the life wasn’t smooth as people are cribbing about. It is written by Adam Smith that Human wants are endless and thus we know humans can never be satisfied. When we were busy and all the work and stressful lives, we all wanted breaks, we used t crib and cry about not getting time for family and friends. Not having time for self This Coronavirus outbreak had taught each one of us that : How important it is to be thankful for everything we have in our lives while sitting I just realize that how lucky we are that we do not have to go out amidst this crisis to work and earn bread. How sorry I’m for each the daily wage workers who have no other way but to go out The second lesson is that we all can survive without fast food, we all are chefs and have made a lot of appealing and tasty dishes without spending a huge and bogus amount. The third lesson is family is everything, during this quarantine I realized that spending time with my family is so stress-busting and helps me be creative. We need to have other hobbies than going out and chilling. We need to look at the weather, it’s April and it is not that scorching heat we used to face, it’s lovely, nature is recovering. Whenever I wake up in the morning I go to my terrace for Yoga and find many families there, walking, gossiping, it has brought everyone close. Mended relation. All the Instagram stories make me realize that we all have talent just we don’t have time to self introspect, we all are running in a race to be the best. From an economic point of view, though savings are leakage in the money flow but still savings are important for unforeseen circumstances. This lockdown has given us all time to reconnect and get together with all our loved ones, to do what we want. The main lesson is that we should enjoy life and every moment as it is. We never know what might happen to anyone. We never know who could be the last person we are talking to, last person we text, let us all be nice to each other There are many lessons that we have learned. One of them is the awareness about the cleanliness. The term CLEANLINESS, that took Modi Ji about 5 years to teach people about Swachh Bharat Abhiyan, a single case of the virus has taught everyone the importance of cleanliness and hygiene. Corona Virus crisis has taught how the humans at halt can lead the nature to work at itself. Nature has its own mysteries and how what the humans have done to its beings is done to them, they are too imprisoned like the animals they have had imprisoned for years away from their homelands. This Crisis has gaped the differences between the communities and brought the Humans together. We can see a wave of excitement in everyone when Modi Ji gives his tasks. Still, remember how my colony just rang up with the sound of bells and plates and claps. And how beautiful it looked with the balconies lit up with Diyas and Candles. It looked like Diwali in the month of April but Diwali with no pollution and noise. This crisis has made everyone of us thankful to the medical and nursing staff who had been working 24×7 hours , away from their families , so that we can stay safe with our families . It has just made me realize that we never know , if we’ll see the sun tomorrow , If we’ll see the people we love , if we can talk to them again , so let’s not hold grudges and let the go and flow . Corona Virus has united all the humanity against a cause to defeat a virus, this virus has made the world a better place, there are no terror attacks, there are no rape cases, there are no murders, no loots. The Earth is all healing. Blackcoffer Insights 17:- Kanika Gusain, Gargi College \n",
      " So not beginning with once upon a time because it is not a fairytale Before the advent of this deadly virus called Coronavirus or with all love that we might want to call it COVID19 the life wasn’t smooth as people are cribbing about It is written by Adam Smith that Human wants are endless and thus we know humans can never be satisfied When we were busy and all the work and stressful lives we all wanted breaks we used t crib and cry about not getting time for family and friends Not having time for self This Coronavirus outbreak had taught each one of us that  How important it is to be thankful for everything we have in our lives while sitting I just realize that how lucky we are that we do not have to go out amidst this crisis to work and earn bread How sorry I’m for each the daily wage workers who have no other way but to go out The second lesson is that we all can survive without fast food we all are chefs and have made a lot of appealing and tasty dishes without spending a huge and bogus amount The third lesson is family is everything during this quarantine I realized that spending time with my family is so stressbusting and helps me be creative We need to have other hobbies than going out and chilling We need to look at the weather it’s April and it is not that scorching heat we used to face it’s lovely nature is recovering Whenever I wake up in the morning I go to my terrace for Yoga and find many families there walking gossiping it has brought everyone close Mended relation All the Instagram stories make me realize that we all have talent just we don’t have time to self introspect we all are running in a race to be the best From an economic point of view though savings are leakage in the money flow but still savings are important for unforeseen circumstances This lockdown has given us all time to reconnect and get together with all our loved ones to do what we want The main lesson is that we should enjoy life and every moment as it is We never know what might happen to anyone We never know who could be the last person we are talking to last person we text let us all be nice to each other There are many lessons that we have learned One of them is the awareness about the cleanliness The term CLEANLINESS that took Modi Ji about 5 years to teach people about Swachh Bharat Abhiyan a single case of the virus has taught everyone the importance of cleanliness and hygiene Corona Virus crisis has taught how the humans at halt can lead the nature to work at itself Nature has its own mysteries and how what the humans have done to its beings is done to them they are too imprisoned like the animals they have had imprisoned for years away from their homelands This Crisis has gaped the differences between the communities and brought the Humans together We can see a wave of excitement in everyone when Modi Ji gives his tasks Still remember how my colony just rang up with the sound of bells and plates and claps And how beautiful it looked with the balconies lit up with Diyas and Candles It looked like Diwali in the month of April but Diwali with no pollution and noise This crisis has made everyone of us thankful to the medical and nursing staff who had been working 24×7 hours  away from their families  so that we can stay safe with our families  It has just made me realize that we never know  if we’ll see the sun tomorrow  If we’ll see the people we love  if we can talk to them again  so let’s not hold grudges and let the go and flow  Corona Virus has united all the humanity against a cause to defeat a virus this virus has made the world a better place there are no terror attacks there are no rape cases there are no murders no loots The Earth is all healing Blackcoffer Insights 17 Kanika Gusain Gargi College \n",
      "['So', 'not', 'beginning', 'with', 'once', 'upon', 'a', 'time', 'because', 'it', 'is', 'not', 'a', 'fairytale', 'Before', 'the', 'advent', 'of', 'this', 'deadly', 'virus', 'called', 'Coronavirus', 'or', 'with', 'all', 'love', 'that', 'we', 'might', 'want', 'to', 'call', 'it', 'COVID19', 'the', 'life', 'wasn’t', 'smooth', 'as', 'people', 'are', 'cribbing', 'about', 'It', 'is', 'written', 'by', 'Adam', 'Smith', 'that', 'Human', 'wants', 'are', 'endless', 'and', 'thus', 'we', 'know', 'humans', 'can', 'never', 'be', 'satisfied', 'When', 'we', 'were', 'busy', 'and', 'all', 'the', 'work', 'and', 'stressful', 'lives', 'we', 'all', 'wanted', 'breaks', 'we', 'used', 't', 'crib', 'and', 'cry', 'about', 'not', 'getting', 'time', 'for', 'family', 'and', 'friends', 'Not', 'having', 'time', 'for', 'self', 'This', 'Coronavirus', 'outbreak', 'had', 'taught', 'each', 'one', 'of', 'us', 'that', 'How', 'important', 'it', 'is', 'to', 'be', 'thankful', 'for', 'everything', 'we', 'have', 'in', 'our', 'lives', 'while', 'sitting', 'I', 'just', 'realize', 'that', 'how', 'lucky', 'we', 'are', 'that', 'we', 'do', 'not', 'have', 'to', 'go', 'out', 'amidst', 'this', 'crisis', 'to', 'work', 'and', 'earn', 'bread', 'How', 'sorry', 'I’m', 'for', 'each', 'the', 'daily', 'wage', 'workers', 'who', 'have', 'no', 'other', 'way', 'but', 'to', 'go', 'out', 'The', 'second', 'lesson', 'is', 'that', 'we', 'all', 'can', 'survive', 'without', 'fast', 'food', 'we', 'all', 'are', 'chefs', 'and', 'have', 'made', 'a', 'lot', 'of', 'appealing', 'and', 'tasty', 'dishes', 'without', 'spending', 'a', 'huge', 'and', 'bogus', 'amount', 'The', 'third', 'lesson', 'is', 'family', 'is', 'everything', 'during', 'this', 'quarantine', 'I', 'realized', 'that', 'spending', 'time', 'with', 'my', 'family', 'is', 'so', 'stressbusting', 'and', 'helps', 'me', 'be', 'creative', 'We', 'need', 'to', 'have', 'other', 'hobbies', 'than', 'going', 'out', 'and', 'chilling', 'We', 'need', 'to', 'look', 'at', 'the', 'weather', 'it’s', 'April', 'and', 'it', 'is', 'not', 'that', 'scorching', 'heat', 'we', 'used', 'to', 'face', 'it’s', 'lovely', 'nature', 'is', 'recovering', 'Whenever', 'I', 'wake', 'up', 'in', 'the', 'morning', 'I', 'go', 'to', 'my', 'terrace', 'for', 'Yoga', 'and', 'find', 'many', 'families', 'there', 'walking', 'gossiping', 'it', 'has', 'brought', 'everyone', 'close', 'Mended', 'relation', 'All', 'the', 'Instagram', 'stories', 'make', 'me', 'realize', 'that', 'we', 'all', 'have', 'talent', 'just', 'we', 'don’t', 'have', 'time', 'to', 'self', 'introspect', 'we', 'all', 'are', 'running', 'in', 'a', 'race', 'to', 'be', 'the', 'best', 'From', 'an', 'economic', 'point', 'of', 'view', 'though', 'savings', 'are', 'leakage', 'in', 'the', 'money', 'flow', 'but', 'still', 'savings', 'are', 'important', 'for', 'unforeseen', 'circumstances', 'This', 'lockdown', 'has', 'given', 'us', 'all', 'time', 'to', 'reconnect', 'and', 'get', 'together', 'with', 'all', 'our', 'loved', 'ones', 'to', 'do', 'what', 'we', 'want', 'The', 'main', 'lesson', 'is', 'that', 'we', 'should', 'enjoy', 'life', 'and', 'every', 'moment', 'as', 'it', 'is', 'We', 'never', 'know', 'what', 'might', 'happen', 'to', 'anyone', 'We', 'never', 'know', 'who', 'could', 'be', 'the', 'last', 'person', 'we', 'are', 'talking', 'to', 'last', 'person', 'we', 'text', 'let', 'us', 'all', 'be', 'nice', 'to', 'each', 'other', 'There', 'are', 'many', 'lessons', 'that', 'we', 'have', 'learned', 'One', 'of', 'them', 'is', 'the', 'awareness', 'about', 'the', 'cleanliness', 'The', 'term', 'CLEANLINESS', 'that', 'took', 'Modi', 'Ji', 'about', '5', 'years', 'to', 'teach', 'people', 'about', 'Swachh', 'Bharat', 'Abhiyan', 'a', 'single', 'case', 'of', 'the', 'virus', 'has', 'taught', 'everyone', 'the', 'importance', 'of', 'cleanliness', 'and', 'hygiene', 'Corona', 'Virus', 'crisis', 'has', 'taught', 'how', 'the', 'humans', 'at', 'halt', 'can', 'lead', 'the', 'nature', 'to', 'work', 'at', 'itself', 'Nature', 'has', 'its', 'own', 'mysteries', 'and', 'how', 'what', 'the', 'humans', 'have', 'done', 'to', 'its', 'beings', 'is', 'done', 'to', 'them', 'they', 'are', 'too', 'imprisoned', 'like', 'the', 'animals', 'they', 'have', 'had', 'imprisoned', 'for', 'years', 'away', 'from', 'their', 'homelands', 'This', 'Crisis', 'has', 'gaped', 'the', 'differences', 'between', 'the', 'communities', 'and', 'brought', 'the', 'Humans', 'together', 'We', 'can', 'see', 'a', 'wave', 'of', 'excitement', 'in', 'everyone', 'when', 'Modi', 'Ji', 'gives', 'his', 'tasks', 'Still', 'remember', 'how', 'my', 'colony', 'just', 'rang', 'up', 'with', 'the', 'sound', 'of', 'bells', 'and', 'plates', 'and', 'claps', 'And', 'how', 'beautiful', 'it', 'looked', 'with', 'the', 'balconies', 'lit', 'up', 'with', 'Diyas', 'and', 'Candles', 'It', 'looked', 'like', 'Diwali', 'in', 'the', 'month', 'of', 'April', 'but', 'Diwali', 'with', 'no', 'pollution', 'and', 'noise', 'This', 'crisis', 'has', 'made', 'everyone', 'of', 'us', 'thankful', 'to', 'the', 'medical', 'and', 'nursing', 'staff', 'who', 'had', 'been', 'working', '24×7', 'hours', 'away', 'from', 'their', 'families', 'so', 'that', 'we', 'can', 'stay', 'safe', 'with', 'our', 'families', 'It', 'has', 'just', 'made', 'me', 'realize', 'that', 'we', 'never', 'know', 'if', 'we’ll', 'see', 'the', 'sun', 'tomorrow', 'If', 'we’ll', 'see', 'the', 'people', 'we', 'love', 'if', 'we', 'can', 'talk', 'to', 'them', 'again', 'so', 'let’s', 'not', 'hold', 'grudges', 'and', 'let', 'the', 'go', 'and', 'flow', 'Corona', 'Virus', 'has', 'united', 'all', 'the', 'humanity', 'against', 'a', 'cause', 'to', 'defeat', 'a', 'virus', 'this', 'virus', 'has', 'made', 'the', 'world', 'a', 'better', 'place', 'there', 'are', 'no', 'terror', 'attacks', 'there', 'are', 'no', 'rape', 'cases', 'there', 'are', 'no', 'murders', 'no', 'loots', 'The', 'Earth', 'is', 'all', 'healing', 'Blackcoffer', 'Insights', '17', 'Kanika', 'Gusain', 'Gargi', 'College']\n"
     ]
    }
   ],
   "source": [
    "content=soup.findAll(attrs={'class':'td-post-content'})\n",
    "content=content[0].text.replace('\\n',\" \")\n",
    "print(content)\n",
    "#Punctuation\n",
    "content = content.translate(str.maketrans('', '', string.punctuation)) \n",
    "print(content)\n",
    "text = content.split()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4435977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2030b390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "#Positive Score \n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\positive-words.txt\") as pos:\n",
    "    poswords = pos.read().split(\"\\n\")  \n",
    "    poswords = poswords[5:]\n",
    "pos_count = \" \".join ([w for w in text if w in poswords])\n",
    "pos_count=pos_count.split(\" \")\n",
    "Positive_score=len(pos_count)\n",
    "print(Positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d024fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "#Negative Score\n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\negative-words.txt\",encoding =\"ISO-8859-1\") as neg:\n",
    "    negwords = neg.read().split(\"\\n\")\n",
    "    \n",
    "negwords = negwords[36:]\n",
    "neg_count = \" \".join ([w for w in text if w in negwords])\n",
    "neg_count=neg_count.split(\" \")\n",
    "Negative_score=len(neg_count)\n",
    "print(Negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff5e022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>filter_content</th>\n",
       "      <th>Positive_Score</th>\n",
       "      <th>Negative_Score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/lessons-from-...</td>\n",
       "      <td>Lessons from the past: Some key learnings rele...</td>\n",
       "      <td>So not beginning with once upon a time becaus...</td>\n",
       "      <td>So not beginning with once upon a time because...</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>0.256335</td>\n",
       "      <td>0.519983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://insights.blackcoffer.com/lessons-from-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Lessons from the past: Some key learnings rele...   \n",
       "\n",
       "                                             content  \\\n",
       "0   So not beginning with once upon a time becaus...   \n",
       "\n",
       "                                      filter_content  Positive_Score  \\\n",
       "0  So not beginning with once upon a time because...              33   \n",
       "\n",
       "   Negative_Score  polarity  subjectivity  \n",
       "0              22  0.256335      0.519983  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_content = ' '.join(text)\n",
    "data=[[url,title,content,filter_content,Positive_score,Negative_score]]\n",
    "data=pd.DataFrame(data,columns=[\"url\",\"title\",\"content\",\"filter_content\",\"Positive_Score\",\"Negative_Score\"])\n",
    "from textblob import TextBlob\n",
    "# Get The Subjectivity\n",
    "def sentiment_analysis(data):\n",
    "    sentiment = TextBlob(data[\"content\"]).sentiment\n",
    "    return pd.Series([sentiment.polarity,sentiment.subjectivity ])\n",
    "\n",
    "# Adding Subjectivity & Polarity\n",
    "data[[\"polarity\", \"subjectivity\"]] = data.apply(sentiment_analysis, axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0ff7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word average = 3041.0\n",
      "FOG INDEX =  288.02\n",
      "Average no of words per sentence\n",
      "716.0\n",
      "Complex Words 1092\n"
     ]
    }
   ],
   "source": [
    "#AVG SENTENCE LENGTH\n",
    "AVG_SENTENCE_LENGTH = len(content.replace(' ',''))/len(re.split(r'[?!.]', content))\n",
    "print('Word average =', AVG_SENTENCE_LENGTH)\n",
    "import textstat\n",
    "#Fog index \n",
    "FOG_INDEX=(textstat.gunning_fog(content))\n",
    "print(\"FOG INDEX = \",FOG_INDEX)\n",
    "#Average No of Words Per Sentence \n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "print(\"Average no of words per sentence\")\n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE=print(sum(AVG_NUMBER_OF_WORDS_PER_SENTENCE)/len(AVG_NUMBER_OF_WORDS_PER_SENTENCE))\n",
    "#Complex words\n",
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    vowels = \"AEIOUYaeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)): \n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"es\"or \"ed\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "COMPLEX_WORDS=syllable_count(content)\n",
    "print(\"Complex Words\",COMPLEX_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0f7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count 3767\n",
      "Percentage of Complex Words 28.988585080966285\n",
      "Average Word per Length 4.2472067039106145\n",
      "The AVG number of syllables in the word is: \n",
      "1.696927374301676\n",
      "Word Count 3767\n",
      "Percentage of Complex Words 28.988585080966285\n",
      "Average Word per Length 4.2472067039106145\n",
      "The AVG number of syllables in the word is: \n",
      "1.696927374301676\n"
     ]
    }
   ],
   "source": [
    "#Word Count\n",
    "Word_Count=len(content)\n",
    "print(\"Word Count\",Word_Count)\n",
    "#Percentage Complex Words\n",
    "pcw=(COMPLEX_WORDS/Word_Count)*100\n",
    "print(\"Percentage of Complex Words\",pcw)\n",
    "#Average Word Length\n",
    "Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "print(\"Average Word per Length\",Average_Word_Length)\n",
    "#Syllable Count Per Word\n",
    "word=content.replace(' ','')\n",
    "syllable_count=0\n",
    "for w in word:\n",
    "      if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "            syllable_count=syllable_count+1\n",
    "print(\"The AVG number of syllables in the word is: \")\n",
    "print(syllable_count/len(content.split()))#Word Count\n",
    "Word_Count=len(content)\n",
    "print(\"Word Count\",Word_Count)\n",
    "#Percentage Complex Words\n",
    "pcw=(COMPLEX_WORDS/Word_Count)*100\n",
    "print(\"Percentage of Complex Words\",pcw)\n",
    "#Average Word Length\n",
    "Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "print(\"Average Word per Length\",Average_Word_Length)\n",
    "#Syllable Count Per Word\n",
    "word=content.replace(' ','')\n",
    "syllable_count=0\n",
    "for w in word:\n",
    "      if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "            syllable_count=syllable_count+1\n",
    "print(\"The AVG number of syllables in the word is: \")\n",
    "print(syllable_count/len(content.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb0990cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "22\n",
      "Word average = 3041.0\n",
      "FOG INDEX =  288.02\n",
      "Average no of words per sentence\n",
      "716.0\n",
      "Complex Words 1092\n",
      "Word Count 3767\n",
      "Percentage of Complex Words 28.988585080966285\n",
      "Average Word per Length 4.2472067039106145\n",
      "The AVG number of syllables in the word is: \n",
      "1.696927374301676\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>filter_content</th>\n",
       "      <th>Positive_Score</th>\n",
       "      <th>Negative_Score</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>Percentage_Complex_Word</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>AVG_NUMBER_OF_WORDS_PER_SENTENCE</th>\n",
       "      <th>COMPLEX_WORDS</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>syllable</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/lessons-from-...</td>\n",
       "      <td>Lessons from the past: Some key learnings rele...</td>\n",
       "      <td>So not beginning with once upon a time becaus...</td>\n",
       "      <td>So not beginning with once upon a time because...</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>3041.0</td>\n",
       "      <td>28.988585</td>\n",
       "      <td>288.02</td>\n",
       "      <td>716.0</td>\n",
       "      <td>1092</td>\n",
       "      <td>3767</td>\n",
       "      <td>1.696927</td>\n",
       "      <td>4.247207</td>\n",
       "      <td>0.256335</td>\n",
       "      <td>0.519983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://insights.blackcoffer.com/lessons-from-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Lessons from the past: Some key learnings rele...   \n",
       "\n",
       "                                             content  \\\n",
       "0   So not beginning with once upon a time becaus...   \n",
       "\n",
       "                                      filter_content  Positive_Score  \\\n",
       "0  So not beginning with once upon a time because...              33   \n",
       "\n",
       "   Negative_Score  Avg_Sentence_Length  Percentage_Complex_Word  Fog_Index  \\\n",
       "0              22               3041.0                28.988585     288.02   \n",
       "\n",
       "    AVG_NUMBER_OF_WORDS_PER_SENTENCE  COMPLEX_WORDS  Word_Count  syllable  \\\n",
       "0                              716.0           1092        3767  1.696927   \n",
       "\n",
       "   Average_Word_Length  polarity  subjectivity  \n",
       "0             4.247207  0.256335      0.519983  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "url = \"\"\"https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis/\"\"\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"}\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "title=soup.find('h1',class_=\"entry-title\")\n",
    "title=title.text.replace('\\n',\" \")\n",
    "# title\n",
    "\n",
    "content=soup.findAll(attrs={'class':'td-post-content'})\n",
    "content=content[0].text.replace('\\n',\" \")\n",
    "# print(content)\n",
    "#Punctuation\n",
    "content = content.translate(str.maketrans('', '', string.punctuation)) \n",
    "# print(content)\n",
    "text = content.split()\n",
    "# print(text)\n",
    "len(text)\n",
    "#Positive Score \n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\positive-words.txt\") as pos:\n",
    "    poswords = pos.read().split(\"\\n\")  \n",
    "    poswords = poswords[5:]\n",
    "pos_count = \" \".join ([w for w in text if w in poswords])\n",
    "pos_count=pos_count.split(\" \")\n",
    "Positive_score=len(pos_count)\n",
    "print(Positive_score)\n",
    "\n",
    "#Negative Score\n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\negative-words.txt\",encoding =\"ISO-8859-1\") as neg:\n",
    "    negwords = neg.read().split(\"\\n\")\n",
    "    \n",
    "negwords = negwords[36:]\n",
    "neg_count = \" \".join ([w for w in text if w in negwords])\n",
    "neg_count=neg_count.split(\" \")\n",
    "Negative_score=len(neg_count)\n",
    "print(Negative_score)\n",
    "\n",
    "\n",
    "filter_content = ' '.join(text)\n",
    "data=[[url,title,content,filter_content,Positive_score,Negative_score]]\n",
    "data=pd.DataFrame(data,columns=[\"url\",\"title\",\"content\",\"filter_content\",\"Positive_Score\",\"Negative_Score\"])\n",
    "from textblob import TextBlob\n",
    "# Get The Subjectivity\n",
    "def sentiment_analysis(data):\n",
    "    sentiment = TextBlob(data[\"content\"]).sentiment\n",
    "    return pd.Series([sentiment.polarity,sentiment.subjectivity ])\n",
    "data[[\"polarity\", \"subjectivity\"]] = data.apply(sentiment_analysis, axis=1)\n",
    "data\n",
    "#AVG SENTENCE LENGTH\n",
    "AVG_SENTENCE_LENGTH = len(content.replace(' ',''))/len(re.split(r'[?!.]', content))\n",
    "print('Word average =', AVG_SENTENCE_LENGTH)\n",
    "import textstat\n",
    "#Fog index \n",
    "FOG_INDEX=(textstat.gunning_fog(content))\n",
    "print(\"FOG INDEX = \",FOG_INDEX)\n",
    "#Average No of Words Per Sentence \n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "print(\"Average no of words per sentence\")\n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE=(sum(AVG_NUMBER_OF_WORDS_PER_SENTENCE)/len(AVG_NUMBER_OF_WORDS_PER_SENTENCE))\n",
    "print(AVG_NUMBER_OF_WORDS_PER_SENTENCE)\n",
    "#Complex words\n",
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    vowels = \"AEIOUYaeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)): \n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"es\"or \"ed\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "COMPLEX_WORDS=syllable_count(content)\n",
    "print(\"Complex Words\",COMPLEX_WORDS)\n",
    "#Word Count\n",
    "Word_Count=len(content)\n",
    "print(\"Word Count\",Word_Count)\n",
    "#Percentage Complex Words\n",
    "pcw=(COMPLEX_WORDS/Word_Count)*100\n",
    "print(\"Percentage of Complex Words\",pcw)\n",
    "#Average Word Length\n",
    "Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "print(\"Average Word per Length\",Average_Word_Length)\n",
    "#Syllable Count Per Word\n",
    "word=content.replace(' ','')\n",
    "syllable_count=0\n",
    "for w in word:\n",
    "      if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "            syllable_count=syllable_count+1\n",
    "print(\"The AVG number of syllables in the word is: \")\n",
    "syllable = (syllable_count/len(content.split()))\n",
    "print(syllable)\n",
    "\n",
    "data = [[url,title,content,filter_content,Positive_score,Negative_score,AVG_SENTENCE_LENGTH,pcw,FOG_INDEX,\n",
    "         AVG_NUMBER_OF_WORDS_PER_SENTENCE,COMPLEX_WORDS,Word_Count,syllable,Average_Word_Length]]\n",
    "data=pd.DataFrame(data,columns=[\"url\",\"title\",\"content\",\"filter_content\",\"Positive_Score\",\"Negative_Score\",\"Avg_Sentence_Length\"\n",
    "                               ,\"Percentage_Complex_Word\",\"Fog_Index\",\" AVG_NUMBER_OF_WORDS_PER_SENTENCE\",\"COMPLEX_WORDS\",\n",
    "                               \"Word_Count\",\"syllable\",\"Average_Word_Length\"])\n",
    "from textblob import TextBlob\n",
    "# Get The Subjectivity\n",
    "def sentiment_analysis(data):\n",
    "    sentiment = TextBlob(data[\"content\"]).sentiment\n",
    "    return pd.Series([sentiment.polarity,sentiment.subjectivity ])\n",
    "data[[\"polarity\", \"subjectivity\"]] = data.apply(sentiment_analysis, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b9ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\Om Bhandwalkar\\Desktop\\BlackCoffer Assignment\\Output\\url_109.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c79b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
