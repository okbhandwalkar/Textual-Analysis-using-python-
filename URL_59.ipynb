{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155b3115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How Data Analytics and AI are used to halt the COVID-19 Pandemic?'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "url = \"\"\"https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/\"\"\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"}\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "title=soup.find('h1',class_=\"entry-title\")\n",
    "title=title.text.replace('\\n',\" \")\n",
    "title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd4851df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Even though COVID-19 has not yet halted and we are facing the nth wave of the coronavirus outbreak across several countries, most notably the US, India, and Brazil. It is a fact that Data Analytics and AI are the big guns of our artillery in this fight against the COVID-19 pandemic. It has helped us in several stages of this outbreak, like the detection of its first outbreak, vaccine development and manufacturing contact tracing, and future hotspot detection. Some of these interesting applications are discussed in this article. A lesser-known fact is that the COVID-19 outbreak was first detected in Toronto, Canada, nearly 7,230 miles away from the first outbreak, nine days before the WHO issued its warning. It was with the help of Big Data Analytics and AI, more specifically Deep Learnings (DL, a subset of Machine Learning) application in Natural Language Processing (NLP) to analyze text inputs that traced the surge of pneumonia cases in the Wuhan province of China. The specialty of DL algorithms is that they mimic the brain cells called neurons and can identify patterns in Big Data. This DL-backed software is used as inputs, reports from public health organizations, global airline ticketing data, etc. These were used to flag unusual surges and potential spreads of infectious diseases. The next application of Big Data Analytics and AI was in the Research and Development of drugs to halt COVID-19. AI was used to analyze the protein structure of the virus, findings that were significant in the progress of vaccine development. In preliminary studies, it was found that it does not mutate as fast as other viruses such as HIV, which means that a prophylactic vaccine is a better way to proceed rather than a therapy. But there is also some evidence supporting the fact that when we find any kind of cure for it, there is a chance of the virus mutating, which is what happened and major mutations have been found in the UK, Brazil, and South Africa. AI also assisted scientists in rapidly shortlisting a set of already available vaccines that could be effective against the coronavirus. Another interesting application of AI can be found in the selection of the right candidates, i.e. most likely to test positive for testing coronavirus in case of insufficient testing resources. This method was first exercised on Greek borders and was called project EVA. Whenever a traveler wanted to come into Greece, he had to fill out a form known as Passenger Locator Form (PLF) at least 24 hours prior to arrival, containing information on their origin country, demographics, point, and date of entry, and the intended destination. EVA then allocated testing resources according to the size of the set of passengers to be tested. After the test results, if found positive, they are put in quarantine. The results were sent back to the program for real-time learning. The question remains how EVA made allocations, It was found that, statistically, only the origin country and the city were significant factors for screening. Ultimately, from a variety of countries and city pairs, EVA had to predict how many testing resources were to be allocated at each entry point and to particular passengers from a location is technically called the Multi-Armed Bandit (MAB) problem, and the chosen method to solve this problem was an AI algorithm called optimistic Gittins index. This algorithm identified on average 1.85x as many asymptomatic, infected travelers as random surveillance testing, and up to 2-4x as many during peak travel. After the test results, if found positive, they are put in quarantine. Following the collection of significant data through the aforementioned process, after a certain period, policies were made categorizing them separately and imposing restrictions on travelers from the specific location. This EVA as presented above was in operation from August 6th to November 1st processing around 38,500 PLFs each day and testing on an average 18.5% of households entering the country every day. Above mentioned applications just show the tip of the iceberg and there is more to get into some of the other developments to watch for include the use of Image Recognition to identify covid based on x-ray images, the use of Deep Learning to predict the 3-D protein structure associated with COVID-19 and so on. Blackcoffer Insights 27: Aniruddha Surse, NIT Nagpur \n",
      " Even though COVID19 has not yet halted and we are facing the nth wave of the coronavirus outbreak across several countries most notably the US India and Brazil It is a fact that Data Analytics and AI are the big guns of our artillery in this fight against the COVID19 pandemic It has helped us in several stages of this outbreak like the detection of its first outbreak vaccine development and manufacturing contact tracing and future hotspot detection Some of these interesting applications are discussed in this article A lesserknown fact is that the COVID19 outbreak was first detected in Toronto Canada nearly 7230 miles away from the first outbreak nine days before the WHO issued its warning It was with the help of Big Data Analytics and AI more specifically Deep Learnings DL a subset of Machine Learning application in Natural Language Processing NLP to analyze text inputs that traced the surge of pneumonia cases in the Wuhan province of China The specialty of DL algorithms is that they mimic the brain cells called neurons and can identify patterns in Big Data This DLbacked software is used as inputs reports from public health organizations global airline ticketing data etc These were used to flag unusual surges and potential spreads of infectious diseases The next application of Big Data Analytics and AI was in the Research and Development of drugs to halt COVID19 AI was used to analyze the protein structure of the virus findings that were significant in the progress of vaccine development In preliminary studies it was found that it does not mutate as fast as other viruses such as HIV which means that a prophylactic vaccine is a better way to proceed rather than a therapy But there is also some evidence supporting the fact that when we find any kind of cure for it there is a chance of the virus mutating which is what happened and major mutations have been found in the UK Brazil and South Africa AI also assisted scientists in rapidly shortlisting a set of already available vaccines that could be effective against the coronavirus Another interesting application of AI can be found in the selection of the right candidates ie most likely to test positive for testing coronavirus in case of insufficient testing resources This method was first exercised on Greek borders and was called project EVA Whenever a traveler wanted to come into Greece he had to fill out a form known as Passenger Locator Form PLF at least 24 hours prior to arrival containing information on their origin country demographics point and date of entry and the intended destination EVA then allocated testing resources according to the size of the set of passengers to be tested After the test results if found positive they are put in quarantine The results were sent back to the program for realtime learning The question remains how EVA made allocations It was found that statistically only the origin country and the city were significant factors for screening Ultimately from a variety of countries and city pairs EVA had to predict how many testing resources were to be allocated at each entry point and to particular passengers from a location is technically called the MultiArmed Bandit MAB problem and the chosen method to solve this problem was an AI algorithm called optimistic Gittins index This algorithm identified on average 185x as many asymptomatic infected travelers as random surveillance testing and up to 24x as many during peak travel After the test results if found positive they are put in quarantine Following the collection of significant data through the aforementioned process after a certain period policies were made categorizing them separately and imposing restrictions on travelers from the specific location This EVA as presented above was in operation from August 6th to November 1st processing around 38500 PLFs each day and testing on an average 185 of households entering the country every day Above mentioned applications just show the tip of the iceberg and there is more to get into some of the other developments to watch for include the use of Image Recognition to identify covid based on xray images the use of Deep Learning to predict the 3D protein structure associated with COVID19 and so on Blackcoffer Insights 27 Aniruddha Surse NIT Nagpur \n",
      "['Even', 'though', 'COVID19', 'has', 'not', 'yet', 'halted', 'and', 'we', 'are', 'facing', 'the', 'nth', 'wave', 'of', 'the', 'coronavirus', 'outbreak', 'across', 'several', 'countries', 'most', 'notably', 'the', 'US', 'India', 'and', 'Brazil', 'It', 'is', 'a', 'fact', 'that', 'Data', 'Analytics', 'and', 'AI', 'are', 'the', 'big', 'guns', 'of', 'our', 'artillery', 'in', 'this', 'fight', 'against', 'the', 'COVID19', 'pandemic', 'It', 'has', 'helped', 'us', 'in', 'several', 'stages', 'of', 'this', 'outbreak', 'like', 'the', 'detection', 'of', 'its', 'first', 'outbreak', 'vaccine', 'development', 'and', 'manufacturing', 'contact', 'tracing', 'and', 'future', 'hotspot', 'detection', 'Some', 'of', 'these', 'interesting', 'applications', 'are', 'discussed', 'in', 'this', 'article', 'A', 'lesserknown', 'fact', 'is', 'that', 'the', 'COVID19', 'outbreak', 'was', 'first', 'detected', 'in', 'Toronto', 'Canada', 'nearly', '7230', 'miles', 'away', 'from', 'the', 'first', 'outbreak', 'nine', 'days', 'before', 'the', 'WHO', 'issued', 'its', 'warning', 'It', 'was', 'with', 'the', 'help', 'of', 'Big', 'Data', 'Analytics', 'and', 'AI', 'more', 'specifically', 'Deep', 'Learnings', 'DL', 'a', 'subset', 'of', 'Machine', 'Learning', 'application', 'in', 'Natural', 'Language', 'Processing', 'NLP', 'to', 'analyze', 'text', 'inputs', 'that', 'traced', 'the', 'surge', 'of', 'pneumonia', 'cases', 'in', 'the', 'Wuhan', 'province', 'of', 'China', 'The', 'specialty', 'of', 'DL', 'algorithms', 'is', 'that', 'they', 'mimic', 'the', 'brain', 'cells', 'called', 'neurons', 'and', 'can', 'identify', 'patterns', 'in', 'Big', 'Data', 'This', 'DLbacked', 'software', 'is', 'used', 'as', 'inputs', 'reports', 'from', 'public', 'health', 'organizations', 'global', 'airline', 'ticketing', 'data', 'etc', 'These', 'were', 'used', 'to', 'flag', 'unusual', 'surges', 'and', 'potential', 'spreads', 'of', 'infectious', 'diseases', 'The', 'next', 'application', 'of', 'Big', 'Data', 'Analytics', 'and', 'AI', 'was', 'in', 'the', 'Research', 'and', 'Development', 'of', 'drugs', 'to', 'halt', 'COVID19', 'AI', 'was', 'used', 'to', 'analyze', 'the', 'protein', 'structure', 'of', 'the', 'virus', 'findings', 'that', 'were', 'significant', 'in', 'the', 'progress', 'of', 'vaccine', 'development', 'In', 'preliminary', 'studies', 'it', 'was', 'found', 'that', 'it', 'does', 'not', 'mutate', 'as', 'fast', 'as', 'other', 'viruses', 'such', 'as', 'HIV', 'which', 'means', 'that', 'a', 'prophylactic', 'vaccine', 'is', 'a', 'better', 'way', 'to', 'proceed', 'rather', 'than', 'a', 'therapy', 'But', 'there', 'is', 'also', 'some', 'evidence', 'supporting', 'the', 'fact', 'that', 'when', 'we', 'find', 'any', 'kind', 'of', 'cure', 'for', 'it', 'there', 'is', 'a', 'chance', 'of', 'the', 'virus', 'mutating', 'which', 'is', 'what', 'happened', 'and', 'major', 'mutations', 'have', 'been', 'found', 'in', 'the', 'UK', 'Brazil', 'and', 'South', 'Africa', 'AI', 'also', 'assisted', 'scientists', 'in', 'rapidly', 'shortlisting', 'a', 'set', 'of', 'already', 'available', 'vaccines', 'that', 'could', 'be', 'effective', 'against', 'the', 'coronavirus', 'Another', 'interesting', 'application', 'of', 'AI', 'can', 'be', 'found', 'in', 'the', 'selection', 'of', 'the', 'right', 'candidates', 'ie', 'most', 'likely', 'to', 'test', 'positive', 'for', 'testing', 'coronavirus', 'in', 'case', 'of', 'insufficient', 'testing', 'resources', 'This', 'method', 'was', 'first', 'exercised', 'on', 'Greek', 'borders', 'and', 'was', 'called', 'project', 'EVA', 'Whenever', 'a', 'traveler', 'wanted', 'to', 'come', 'into', 'Greece', 'he', 'had', 'to', 'fill', 'out', 'a', 'form', 'known', 'as', 'Passenger', 'Locator', 'Form', 'PLF', 'at', 'least', '24', 'hours', 'prior', 'to', 'arrival', 'containing', 'information', 'on', 'their', 'origin', 'country', 'demographics', 'point', 'and', 'date', 'of', 'entry', 'and', 'the', 'intended', 'destination', 'EVA', 'then', 'allocated', 'testing', 'resources', 'according', 'to', 'the', 'size', 'of', 'the', 'set', 'of', 'passengers', 'to', 'be', 'tested', 'After', 'the', 'test', 'results', 'if', 'found', 'positive', 'they', 'are', 'put', 'in', 'quarantine', 'The', 'results', 'were', 'sent', 'back', 'to', 'the', 'program', 'for', 'realtime', 'learning', 'The', 'question', 'remains', 'how', 'EVA', 'made', 'allocations', 'It', 'was', 'found', 'that', 'statistically', 'only', 'the', 'origin', 'country', 'and', 'the', 'city', 'were', 'significant', 'factors', 'for', 'screening', 'Ultimately', 'from', 'a', 'variety', 'of', 'countries', 'and', 'city', 'pairs', 'EVA', 'had', 'to', 'predict', 'how', 'many', 'testing', 'resources', 'were', 'to', 'be', 'allocated', 'at', 'each', 'entry', 'point', 'and', 'to', 'particular', 'passengers', 'from', 'a', 'location', 'is', 'technically', 'called', 'the', 'MultiArmed', 'Bandit', 'MAB', 'problem', 'and', 'the', 'chosen', 'method', 'to', 'solve', 'this', 'problem', 'was', 'an', 'AI', 'algorithm', 'called', 'optimistic', 'Gittins', 'index', 'This', 'algorithm', 'identified', 'on', 'average', '185x', 'as', 'many', 'asymptomatic', 'infected', 'travelers', 'as', 'random', 'surveillance', 'testing', 'and', 'up', 'to', '24x', 'as', 'many', 'during', 'peak', 'travel', 'After', 'the', 'test', 'results', 'if', 'found', 'positive', 'they', 'are', 'put', 'in', 'quarantine', 'Following', 'the', 'collection', 'of', 'significant', 'data', 'through', 'the', 'aforementioned', 'process', 'after', 'a', 'certain', 'period', 'policies', 'were', 'made', 'categorizing', 'them', 'separately', 'and', 'imposing', 'restrictions', 'on', 'travelers', 'from', 'the', 'specific', 'location', 'This', 'EVA', 'as', 'presented', 'above', 'was', 'in', 'operation', 'from', 'August', '6th', 'to', 'November', '1st', 'processing', 'around', '38500', 'PLFs', 'each', 'day', 'and', 'testing', 'on', 'an', 'average', '185', 'of', 'households', 'entering', 'the', 'country', 'every', 'day', 'Above', 'mentioned', 'applications', 'just', 'show', 'the', 'tip', 'of', 'the', 'iceberg', 'and', 'there', 'is', 'more', 'to', 'get', 'into', 'some', 'of', 'the', 'other', 'developments', 'to', 'watch', 'for', 'include', 'the', 'use', 'of', 'Image', 'Recognition', 'to', 'identify', 'covid', 'based', 'on', 'xray', 'images', 'the', 'use', 'of', 'Deep', 'Learning', 'to', 'predict', 'the', '3D', 'protein', 'structure', 'associated', 'with', 'COVID19', 'and', 'so', 'on', 'Blackcoffer', 'Insights', '27', 'Aniruddha', 'Surse', 'NIT', 'Nagpur']\n"
     ]
    }
   ],
   "source": [
    "content=soup.findAll(attrs={'class':'td-post-content'})\n",
    "content=content[0].text.replace('\\n',\" \")\n",
    "print(content)\n",
    "#Punctuation\n",
    "content = content.translate(str.maketrans('', '', string.punctuation)) \n",
    "print(content)\n",
    "text = content.split()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf280d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0364c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "#Positive Score \n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\positive-words.txt\") as pos:\n",
    "    poswords = pos.read().split(\"\\n\")  \n",
    "    poswords = poswords[5:]\n",
    "pos_count = \" \".join ([w for w in text if w in poswords])\n",
    "pos_count=pos_count.split(\" \")\n",
    "Positive_score=len(pos_count)\n",
    "print(Positive_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a559dd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "#Negative Score\n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\negative-words.txt\",encoding =\"ISO-8859-1\") as neg:\n",
    "    negwords = neg.read().split(\"\\n\")\n",
    "    \n",
    "negwords = negwords[36:]\n",
    "neg_count = \" \".join ([w for w in text if w in negwords])\n",
    "neg_count=neg_count.split(\" \")\n",
    "Negative_score=len(neg_count)\n",
    "print(Negative_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a35d37b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>filter_content</th>\n",
       "      <th>Positive_Score</th>\n",
       "      <th>Negative_Score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/how-data-anal...</td>\n",
       "      <td>How Data Analytics and AI are used to halt the...</td>\n",
       "      <td>Even though COVID19 has not yet halted and we...</td>\n",
       "      <td>Even though COVID19 has not yet halted and we ...</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>0.155938</td>\n",
       "      <td>0.410028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://insights.blackcoffer.com/how-data-anal...   \n",
       "\n",
       "                                               title  \\\n",
       "0  How Data Analytics and AI are used to halt the...   \n",
       "\n",
       "                                             content  \\\n",
       "0   Even though COVID19 has not yet halted and we...   \n",
       "\n",
       "                                      filter_content  Positive_Score  \\\n",
       "0  Even though COVID19 has not yet halted and we ...              21   \n",
       "\n",
       "   Negative_Score  polarity  subjectivity  \n",
       "0              14  0.155938      0.410028  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_content = ' '.join(text)\n",
    "data=[[url,title,content,filter_content,Positive_score,Negative_score]]\n",
    "data=pd.DataFrame(data,columns=[\"url\",\"title\",\"content\",\"filter_content\",\"Positive_Score\",\"Negative_Score\"])\n",
    "from textblob import TextBlob\n",
    "# Get The Subjectivity\n",
    "def sentiment_analysis(data):\n",
    "    sentiment = TextBlob(data[\"content\"]).sentiment\n",
    "    return pd.Series([sentiment.polarity,sentiment.subjectivity ])\n",
    "\n",
    "# Adding Subjectivity & Polarity\n",
    "data[[\"polarity\", \"subjectivity\"]] = data.apply(sentiment_analysis, axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3edb9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word average = 3573.0\n",
      "FOG INDEX =  292.61\n",
      "Average no of words per sentence\n",
      "720.0\n",
      "Complex Words 1281\n"
     ]
    }
   ],
   "source": [
    "#AVG SENTENCE LENGTH\n",
    "AVG_SENTENCE_LENGTH = len(content.replace(' ',''))/len(re.split(r'[?!.]', content))\n",
    "print('Word average =', AVG_SENTENCE_LENGTH)\n",
    "import textstat\n",
    "#Fog index \n",
    "FOG_INDEX=(textstat.gunning_fog(content))\n",
    "print(\"FOG INDEX = \",FOG_INDEX)\n",
    "#Average No of Words Per Sentence \n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "print(\"Average no of words per sentence\")\n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE=print(sum(AVG_NUMBER_OF_WORDS_PER_SENTENCE)/len(AVG_NUMBER_OF_WORDS_PER_SENTENCE))\n",
    "#Complex words\n",
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    vowels = \"AEIOUYaeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)): \n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"es\"or \"ed\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "COMPLEX_WORDS=syllable_count(content)\n",
    "print(\"Complex Words\",COMPLEX_WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a0aa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count 4294\n",
      "Percentage of Complex Words 29.83232417326502\n",
      "Average Word per Length 4.9625\n",
      "The AVG number of syllables in the word is: \n",
      "1.9722222222222223\n"
     ]
    }
   ],
   "source": [
    "#Word Count\n",
    "Word_Count=len(content)\n",
    "print(\"Word Count\",Word_Count)\n",
    "#Percentage Complex Words\n",
    "pcw=(COMPLEX_WORDS/Word_Count)*100\n",
    "print(\"Percentage of Complex Words\",pcw)\n",
    "#Average Word Length\n",
    "Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "print(\"Average Word per Length\",Average_Word_Length)\n",
    "#Syllable Count Per Word\n",
    "word=content.replace(' ','')\n",
    "syllable_count=0\n",
    "for w in word:\n",
    "      if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "            syllable_count=syllable_count+1\n",
    "print(\"The AVG number of syllables in the word is: \")\n",
    "print(syllable_count/len(content.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0552ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "14\n",
      "Word average = 3573.0\n",
      "FOG INDEX =  292.61\n",
      "Average no of words per sentence\n",
      "720.0\n",
      "Complex Words 1281\n",
      "Word Count 4294\n",
      "Percentage of Complex Words 29.83232417326502\n",
      "Average Word per Length 4.9625\n",
      "The AVG number of syllables in the word is: \n",
      "1.9722222222222223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>filter_content</th>\n",
       "      <th>Positive_Score</th>\n",
       "      <th>Negative_Score</th>\n",
       "      <th>Avg_Sentence_Length</th>\n",
       "      <th>Percentage_Complex_Word</th>\n",
       "      <th>Fog_Index</th>\n",
       "      <th>AVG_NUMBER_OF_WORDS_PER_SENTENCE</th>\n",
       "      <th>COMPLEX_WORDS</th>\n",
       "      <th>Word_Count</th>\n",
       "      <th>syllable</th>\n",
       "      <th>Average_Word_Length</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://insights.blackcoffer.com/how-data-anal...</td>\n",
       "      <td>How Data Analytics and AI are used to halt the...</td>\n",
       "      <td>Even though COVID19 has not yet halted and we...</td>\n",
       "      <td>Even though COVID19 has not yet halted and we ...</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>3573.0</td>\n",
       "      <td>29.832324</td>\n",
       "      <td>292.61</td>\n",
       "      <td>720.0</td>\n",
       "      <td>1281</td>\n",
       "      <td>4294</td>\n",
       "      <td>1.972222</td>\n",
       "      <td>4.9625</td>\n",
       "      <td>0.155938</td>\n",
       "      <td>0.410028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://insights.blackcoffer.com/how-data-anal...   \n",
       "\n",
       "                                               title  \\\n",
       "0  How Data Analytics and AI are used to halt the...   \n",
       "\n",
       "                                             content  \\\n",
       "0   Even though COVID19 has not yet halted and we...   \n",
       "\n",
       "                                      filter_content  Positive_Score  \\\n",
       "0  Even though COVID19 has not yet halted and we ...              21   \n",
       "\n",
       "   Negative_Score  Avg_Sentence_Length  Percentage_Complex_Word  Fog_Index  \\\n",
       "0              14               3573.0                29.832324     292.61   \n",
       "\n",
       "    AVG_NUMBER_OF_WORDS_PER_SENTENCE  COMPLEX_WORDS  Word_Count  syllable  \\\n",
       "0                              720.0           1281        4294  1.972222   \n",
       "\n",
       "   Average_Word_Length  polarity  subjectivity  \n",
       "0               4.9625  0.155938      0.410028  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import spacy\n",
    "import re\n",
    "url = \"\"\"https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/\"\"\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"}\n",
    "page = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "soup=BeautifulSoup(page.content, 'html.parser')\n",
    "title=soup.find('h1',class_=\"entry-title\")\n",
    "title=title.text.replace('\\n',\" \")\n",
    "# title\n",
    "\n",
    "content=soup.findAll(attrs={'class':'td-post-content'})\n",
    "content=content[0].text.replace('\\n',\" \")\n",
    "# print(content)\n",
    "#Punctuation\n",
    "content = content.translate(str.maketrans('', '', string.punctuation)) \n",
    "# print(content)\n",
    "text = content.split()\n",
    "# print(text)\n",
    "len(text)\n",
    "#Positive Score \n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\positive-words.txt\") as pos:\n",
    "    poswords = pos.read().split(\"\\n\")  \n",
    "    poswords = poswords[5:]\n",
    "pos_count = \" \".join ([w for w in text if w in poswords])\n",
    "pos_count=pos_count.split(\" \")\n",
    "Positive_score=len(pos_count)\n",
    "print(Positive_score)\n",
    "\n",
    "#Negative Score\n",
    "with open(r\"C:\\Users\\Om Bhandwalkar\\Desktop\\pos\\negative-words.txt\",encoding =\"ISO-8859-1\") as neg:\n",
    "    negwords = neg.read().split(\"\\n\")\n",
    "    \n",
    "negwords = negwords[36:]\n",
    "neg_count = \" \".join ([w for w in text if w in negwords])\n",
    "neg_count=neg_count.split(\" \")\n",
    "Negative_score=len(neg_count)\n",
    "print(Negative_score)\n",
    "\n",
    "\n",
    "filter_content = ' '.join(text)\n",
    "data=[[url,title,content,filter_content,Positive_score,Negative_score]]\n",
    "data=pd.DataFrame(data,columns=[\"url\",\"title\",\"content\",\"filter_content\",\"Positive_Score\",\"Negative_Score\"])\n",
    "from textblob import TextBlob\n",
    "# Get The Subjectivity\n",
    "def sentiment_analysis(data):\n",
    "    sentiment = TextBlob(data[\"content\"]).sentiment\n",
    "    return pd.Series([sentiment.polarity,sentiment.subjectivity ])\n",
    "data[[\"polarity\", \"subjectivity\"]] = data.apply(sentiment_analysis, axis=1)\n",
    "data\n",
    "#AVG SENTENCE LENGTH\n",
    "AVG_SENTENCE_LENGTH = len(content.replace(' ',''))/len(re.split(r'[?!.]', content))\n",
    "print('Word average =', AVG_SENTENCE_LENGTH)\n",
    "import textstat\n",
    "#Fog index \n",
    "FOG_INDEX=(textstat.gunning_fog(content))\n",
    "print(\"FOG INDEX = \",FOG_INDEX)\n",
    "#Average No of Words Per Sentence \n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE = [len(l.split()) for l in re.split(r'[?!.]', content) if l.strip()]\n",
    "print(\"Average no of words per sentence\")\n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE=(sum(AVG_NUMBER_OF_WORDS_PER_SENTENCE)/len(AVG_NUMBER_OF_WORDS_PER_SENTENCE))\n",
    "print(AVG_NUMBER_OF_WORDS_PER_SENTENCE)\n",
    "#Complex words\n",
    "def syllable_count(word):\n",
    "    count = 0\n",
    "    vowels = \"AEIOUYaeiouy\"\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)): \n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "            if word.endswith(\"es\"or \"ed\"):\n",
    "                count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "\n",
    "COMPLEX_WORDS=syllable_count(content)\n",
    "print(\"Complex Words\",COMPLEX_WORDS)\n",
    "#Word Count\n",
    "Word_Count=len(content)\n",
    "print(\"Word Count\",Word_Count)\n",
    "#Percentage Complex Words\n",
    "pcw=(COMPLEX_WORDS/Word_Count)*100\n",
    "print(\"Percentage of Complex Words\",pcw)\n",
    "#Average Word Length\n",
    "Average_Word_Length=len(content.replace(' ',''))/len(content.split())\n",
    "print(\"Average Word per Length\",Average_Word_Length)\n",
    "#Syllable Count Per Word\n",
    "word=content.replace(' ','')\n",
    "syllable_count=0\n",
    "for w in word:\n",
    "      if(w=='a' or w=='e' or w=='i' or w=='o' or w=='y' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U' or w=='Y'):\n",
    "            syllable_count=syllable_count+1\n",
    "print(\"The AVG number of syllables in the word is: \")\n",
    "syllable = (syllable_count/len(content.split()))\n",
    "print(syllable)\n",
    "\n",
    "data = [[url,title,content,filter_content,Positive_score,Negative_score,AVG_SENTENCE_LENGTH,pcw,FOG_INDEX,\n",
    "         AVG_NUMBER_OF_WORDS_PER_SENTENCE,COMPLEX_WORDS,Word_Count,syllable,Average_Word_Length]]\n",
    "data=pd.DataFrame(data,columns=[\"url\",\"title\",\"content\",\"filter_content\",\"Positive_Score\",\"Negative_Score\",\"Avg_Sentence_Length\"\n",
    "                               ,\"Percentage_Complex_Word\",\"Fog_Index\",\" AVG_NUMBER_OF_WORDS_PER_SENTENCE\",\"COMPLEX_WORDS\",\n",
    "                               \"Word_Count\",\"syllable\",\"Average_Word_Length\"])\n",
    "from textblob import TextBlob\n",
    "# Get The Subjectivity\n",
    "def sentiment_analysis(data):\n",
    "    sentiment = TextBlob(data[\"content\"]).sentiment\n",
    "    return pd.Series([sentiment.polarity,sentiment.subjectivity ])\n",
    "data[[\"polarity\", \"subjectivity\"]] = data.apply(sentiment_analysis, axis=1)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fb5c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\Om Bhandwalkar\\Desktop\\BlackCoffer Assignment\\Output\\url_59.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa195eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
